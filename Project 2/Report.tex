\documentclass{article}
\usepackage{ctex}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}

\title{深度学习与神经网络第二次课程项目}
\author{王逸群 19307110397}
\date{2022.4.9}

\definecolor{gray}{rgb}{0.99,0.99,0.99}

\lstdefinestyle{mystyle}{
	basicstyle=\footnotesize,
	backgroundcolor=\color{gray},
	numbers=left
}

\lstset{style=mystyle}

\begin{document}
	
\maketitle

\section{神经网络}

\subsection{初始设置}

本项目使用CIFAR-10数据集，
其中包含60000张$32\times32$的彩色图片，
被平均分为10类：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、货车。

参考VGG网络架构，
基于pytorch框架，
设计神经网络初始架构。
对于输入的图像，
先进行两轮卷积、激活、池化操作，
使图像边长由32变为16再变为8，
图像频道数由3变为16再变为32；
接着进行三轮线性、激活操作，
使神经元数量由32*8*8变为128再变为10。
初始架构的参数数量为285162，
类存储于\verb|Code/nn.py|，
具体内容如下：

\begin{lstlisting}[language=Python]
class NN(nn.Module):
    def __init__(self, in_channels = 3, hidden_channels = (16, 32), 
                 hidden_neurons = (128, 128), num_classes = 10):
        super().__init__()
        self.hidden_channels = hidden_channels

        self.extractor = nn.Sequential(
            # stage 1
            nn.Conv2d(in_channels = in_channels,
                      out_channels = hidden_channels[0],
                      kernel_size = 3, padding = 1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2),

            # stage 2
            nn.Conv2d(in_channels = hidden_channels[0],
                      out_channels = hidden_channels[1],
                      kernel_size = 3, padding = 1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2))

        self.classifier = nn.Sequential(
            nn.Linear(hidden_channels[1] * 8 * 8, hidden_neurons[0]),
            nn.ReLU(),
            nn.Linear(hidden_neurons[0], hidden_neurons[1]),
            nn.ReLU(),
            nn.Linear(hidden_neurons[1], num_classes))

    def forward(self, inputs):
        hidden = self.extractor(inputs)
        outputs = \
            self.classifier(hidden.view(-1,
                                        self.hidden_channels[1] * 8 * 8))
        return outputs
\end{lstlisting}

其余参数的初始设置如下：

损失函数：交叉熵损失函数；

优化器：Adam；

学习率：0.001；

初始设置运行结果如图\ref{fig:Original}所示，
训练集上的最优错误率为0.05860，在第19回合出现；
测试集上的最优错误率为0.30160，在第8回合出现。

\begin{figure}[h]
\includegraphics[width=\textwidth]
{Result/NN Original/figure.png}
\caption{原始模型在测试集和验证集上的错误率}
\label{fig:Original}
\end{figure}

\subsection{参数调整}

\subsubsection{神经元数量}

本节在总体架构不变的基础上，
改变神经元数量，
实验设置如表\ref{table:SizeSet}所示，
结果如表\ref{table:SizeResult}和图\ref{fig:Size}所示。

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|} 
\hline
& \verb|hidden_channels| & \verb|hidden_neurons| & 参数数量\\
\hline
原模型 & \verb|(16,  32)| & \verb|(128,  128)| & 285162 \\
更小的模型 & \verb|( 4,   8)| & \verb|( 32,   32)| & 18210 \\
更大的模型 & \verb|(64, 128)| & \verb|(512,  512)| & 4538250\\
\hline
\end{tabular}
\caption{神经元数量实验设置}
\label{table:SizeSet}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|} 
\hline
& 训练集最优错误率 & 回合 & 测试集最优错误率 & 回合 \\
\hline
原模型 & 0.05860 & 19 & 0.30160 & 8 \\
更小的模型 & 0.37472 & 20 & 0.40570 & 20 \\
更大的模型 & 0.00978 & 19 & 0.26310 & 4 \\
\hline
\end{tabular}
\caption{神经元数量实验结果}
\label{table:SizeResult}
\end{table}

\begin{figure}[p]
\includegraphics[width=\textwidth]
{Result/NN bigger/figure.png}
\caption{神经元数量实验结果}
\label{fig:Size}
\end{figure}

可以看到，随着模型的规模变大，参数数量增加，
训练集和测试集的最优错误率都有所上升，
但是测试集最优错误率的上升幅度非常有限。

\subsubsection{损失函数}

初始设置使用交叉熵损失函数，本节尝试使用多分类的合页损失函数。
实验结果如图\ref{fig:Loss}和表\ref{table:Loss}所示。

\begin{figure}[p]
\includegraphics[width=\textwidth]
{Result/NN hinge/figure.png}
\caption{损失函数实验结果}
\label{fig:Loss}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|} 
\hline
损失函数 & 训练集最优错误率 & 回合 & 测试集最优错误率 & 回合 \\
\hline
交叉熵 & 0.05860 & 19 & 0.30160 & 8 \\
合页 & 0.07886 & 20 & 0.31200 & 15 \\
\hline
\end{tabular}
\caption{损失函数实验结果}
\label{table:Loss}
\end{table}

可以看到，使用多分类的合页损失函数并没有明显的提升效果。

\subsubsection{正则化}

初始设置未加入正则化，本节尝试使用不同的正则化参数，
实验结果如图和表\ref{table:Regularize}所示。

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|} 
\hline
正则化参数 & 训练集最优错误率 & 回合 & 测试集最优错误率 & 回合 \\
\hline
0 & 0.05860 & 19 & 0.30160 & 8 \\
0.05 & 0.74906 & 20 & 0.74940 & 20 \\
0.01 & 0.45312 & 16 & 0.45140 & 20 \\
0.005 & 0.35582 & 17 & 0.37120 & 17 \\
0.0005 &  &  &  &  \\
0.00005 &  &  &  &  \\
\hline
\end{tabular}
\caption{正则化实验结果}
\label{table:Regularize}
\end{table}

\subsubsection{激活函数}

初始设置使用ReLU激活函数，本节尝试使用tanh和softplus激活函数。
实验结果如图和表\ref{table:Activation}所示。

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|} 
\hline
激活函数 & 训练集最优错误率 & 回合 & 测试集最优错误率 & 回合 \\
\hline
ReLU & 0.05860 & 19 & 0.30160 & 8 \\
tanh &  &  &  &  \\
softplus &  &  &  &  \\
\hline
\end{tabular}
\caption{激活函数实验结果}
\label{table:Activation}
\end{table}

\subsubsection{优化器}

初始设置使用Adam优化器，
本节尝试使用SGD优化器、
带有Momentum的SGD优化器、
以及Adagrad优化器，
实验结果如图和表\ref{table:Optimize}所示。

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|} 
\hline
优化器 & 训练集最优错误率 & 回合 & 测试集最优错误率 & 回合 \\
\hline
Adam & 0.05860 & 19 & 0.30160 & 8 \\
SGD &  &  &  &  \\
Momentum &  &  &  &  \\
Adagrad &  &  &  &  \\
\hline
\end{tabular}
\caption{优化器实验结果}
\label{table:Optimize}
\end{table}

\subsubsection{批归一化}

\subsubsection{丢弃法}

\subsubsection{残差连接}

\subsection{可视化}

\subsubsection{卷积核可视化}

\subsubsection{损失函数曲线}

\section{批归一化}
	
\end{document}